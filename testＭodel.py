import requests
import argparse
import json
import sys

# --- è¨­å®š ---
# æ‚¨çš„ vLLM OpenAI API ä¼ºæœå™¨é‹è¡Œçš„ä½å€
VLLM_API_URL = "http://localhost:8000/v1/chat/completions"
# ç²å–æ¨¡å‹åˆ—è¡¨çš„ API ä½å€
VLLM_MODELS_URL = "http://localhost:8000/v1/models"
HEADERS = {"Content-Type": "application/json"}

def get_first_available_model():
    """è‡ªå‹•å¾ vLLM ä¼ºæœå™¨ç²å–ç¬¬ä¸€å€‹å¯ç”¨çš„æ¨¡å‹åç¨±"""
    try:
        response = requests.get(VLLM_MODELS_URL)
        response.raise_for_status()  # å¦‚æœè«‹æ±‚å¤±æ•—å‰‡æ‹‹å‡ºç•°å¸¸
        models = response.json()
        if "data" in models and len(models["data"]) > 0:
            model_name = models["data"][0]["id"]
            print(f"âœ… è‡ªå‹•åµæ¸¬åˆ°æ¨¡å‹: {model_name}")
            return model_name
        else:
            print("âŒ éŒ¯èª¤ï¼šç„¡æ³•å¾ä¼ºæœå™¨ç²å–æ¨¡å‹åˆ—è¡¨ï¼Œè«‹æª¢æŸ¥ä¼ºæœå™¨æ˜¯å¦å·²æ­£ç¢ºåŠ è¼‰æ¨¡å‹ã€‚")
            return None
    except requests.exceptions.RequestException as e:
        print(f"âŒ éŒ¯èª¤ï¼šç„¡æ³•é€£æ¥åˆ° vLLM ä¼ºæœå™¨ at {VLLM_MODELS_URL}ã€‚")
        print("è«‹ç¢ºèªï¼š")
        print("1. vLLM ä¼ºæœå™¨æ˜¯å¦å·²æˆåŠŸå•Ÿå‹•ï¼Ÿ")
        print("2. ä¼ºæœå™¨ä½å€å’ŒåŸ è™Ÿæ˜¯å¦æ­£ç¢ºï¼Ÿ")
        print(f"è©³ç´°éŒ¯èª¤: {e}")
        return None

def query_vllm_once(prompt: str, model_name: str):
    """ç™¼é€è«‹æ±‚ä¸¦ä¸€æ¬¡æ€§ç²å–å®Œæ•´å›è¦†"""
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 1024,
        "temperature": 0.7,
        "stream": False # è¨­ç‚º False
    }

    try:
        response = requests.post(VLLM_API_URL, headers=HEADERS, data=json.dumps(payload))
        response.raise_for_status()
        
        data = response.json()
        content = data['choices'][0]['message']['content']

        print("\n--- vLLM å›æ‡‰ (ä¸€æ¬¡æ€§) ---")
        print(content)
        print("--------------------------\n")

    except requests.exceptions.RequestException as e:
        print(f"âŒ è«‹æ±‚å¤±æ•—: {e}")

def query_vllm_stream(prompt: str, model_name: str):
    """ç™¼é€è«‹æ±‚ä¸¦ä»¥ä¸²æµæ–¹å¼è™•ç†å›è¦†"""
    payload = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 1024,
        "temperature": 0.7,
        "stream": True # è¨­ç‚º True
    }

    try:
        print("\n--- vLLM å›æ‡‰ (ä¸²æµ) ---")
        with requests.post(VLLM_API_URL, headers=HEADERS, data=json.dumps(payload), stream=True) as response:
            response.raise_for_status()
            for chunk in response.iter_lines():
                if chunk:
                    decoded_chunk = chunk.decode('utf-8')
                    # vLLM çš„ä¸²æµè¼¸å‡ºä»¥ "data: " é–‹é ­
                    if decoded_chunk.startswith('data: '):
                        json_str = decoded_chunk[len('data: '):]
                        if json_str.strip() == '[DONE]':
                            break
                        try:
                            data = json.loads(json_str)
                            delta = data['choices'][0]['delta']
                            if 'content' in delta:
                                content = delta['content']
                                # ä½¿ç”¨ end='' å’Œ flush=True å¯¦ç¾å³æ™‚æ‰“å°
                                print(content, end='', flush=True)
                        except json.JSONDecodeError:
                            print(f"\nç„¡æ³•è§£æçš„ JSON ç‰‡æ®µ: {json_str}")
        print("\n--------------------------\n")

    except requests.exceptions.RequestException as e:
        print(f"\nâŒ è«‹æ±‚å¤±æ•—: {e}")

def main():
    parser = argparse.ArgumentParser(description="ä¸€å€‹ç”¨ä¾†æ¸¬è©¦ vLLM OpenAI API çš„ Python è…³æœ¬ã€‚")
    parser.add_argument("prompt", type=str, help="æ‚¨è¦å‘å¤§å‹èªè¨€æ¨¡å‹æå•çš„å•é¡Œã€‚")
    parser.add_argument("--stream", action="store_true", help="ä½¿ç”¨ä¸²æµæ¨¡å¼æ¥æ”¶å›è¦†ã€‚")
    
    args = parser.parse_args()

    model_name = get_first_available_model()
    
    if not model_name:
        sys.exit(1) # å¦‚æœæ²’æœ‰ç²å–åˆ°æ¨¡å‹ï¼Œå‰‡é€€å‡ºè…³æœ¬

    print(f"ğŸ’¬ æ­£åœ¨ç™¼é€å•é¡Œ: \"{args.prompt}\"")

    if args.stream:
        query_vllm_stream(args.prompt, model_name)
    else:
        query_vllm_once(args.prompt, model_name)

if __name__ == "__main__":
    main()