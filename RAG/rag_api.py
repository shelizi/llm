"""
RAG API - ä»¥ FastAPI å»ºç«‹ `/query` ç«¯é»ï¼Œä¾› OpenWebUI æˆ–å…¶ä»–æœå‹™å‘¼å«ã€‚
å•Ÿå‹•å¾Œå°‡è¼‰å…¥ï¼ˆæˆ–å˜—è©¦è¼‰å…¥ï¼‰æ—¢æœ‰çš„ ChromaDB å‘é‡ç´¢å¼•ã€‚
è‹¥æœªæ‰¾åˆ°ç´¢å¼•å°‡å›å‚³ 503 éŒ¯èª¤ï¼Œè«‹å…ˆåŸ·è¡Œç›¸åŒç›®éŒ„ä¸‹çš„ `chroma_llamaindex_example.py`
æˆ–è‡ªè¡Œæ’°å¯«è…³æœ¬å»ºç½®ç´¢å¼•ã€‚

### ä¸»è¦ç«¯é»
POST /query
Body:
{
  "query": "è«‹å•é˜¿é‡Œå±±åœ¨å“ªè£¡ï¼Ÿ",
  "top_k": 5,         # é¸å¡«ï¼Œé è¨­ 5ï¼Œç¯„åœ 1-20
  "filenames": []     # é¸å¡«ï¼ŒæŒ‡å®šè¦æŸ¥è©¢çš„æª”æ¡ˆåç¨±åˆ—è¡¨ï¼Œç©ºåˆ—è¡¨è¡¨ç¤ºæŸ¥è©¢æ‰€æœ‰æª”æ¡ˆ
}

Response:
{
  "answer": "...",
  "sources": [...]
}

GET /indexed_files
Response:
{
  "files": ["file1.txt", "file2.pdf", ...]
}
"""

from pathlib import Path
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
import logging
from typing import List
from huggingface_hub import HfApi
from llama_index.core import Settings  # æ–°å¢

# å…¨åŸŸåœç”¨ LLMï¼Œé¿å…éœ€è¦ OpenAI API KEY
Settings.llm = None
import torch
import chromadb
import os
import asyncio

from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import StorageContext, VectorStoreIndex

# ----------------- åŸºæœ¬è¨­å®š -----------------
app = FastAPI(title="RAG API with LlamaIndex & Chroma")

class QueryRequest(BaseModel):
    query: str
    top_k: int = Field(default=5, ge=1, le=20, description="è¿”å›çš„ç›¸ä¼¼æ–‡æª”æ•¸é‡ï¼Œç¯„åœ1-20")
    filenames: list[str] = Field(default=[], description="æŒ‡å®šè¦æŸ¥è©¢çš„æª”æ¡ˆåç¨±åˆ—è¡¨ï¼Œç©ºåˆ—è¡¨è¡¨ç¤ºæŸ¥è©¢æ‰€æœ‰æª”æ¡ˆ")

class SourceItem(BaseModel):
    filename: str
    snippet: str
    score: float

class QueryResponse(BaseModel):
    answer: str
    sources: list[SourceItem] = []
    enhanced_answer: str = ""  # AIå¢å¼·å›ç­”
    answer_source: str = "retrieval"  # "retrieval" æˆ– "ai_enhanced"

# ç’°å¢ƒè®Šæ•¸ / é è¨­å€¼
CHROMA_DIR = Path(__file__).parent / "chroma_db"
COLLECTION_NAME = "taiwan_demo"  # èˆ‡ build_index ä¸­ä¿æŒä¸€è‡´
EMBED_MODEL_NAME = "intfloat/multilingual-e5-large-instruct"

# é å…ˆæª¢æŸ¥/ä¸‹è¼‰çš„åµŒå…¥æ¨¡å‹æ¸…å–® (å¯æ–¼å…¶ä»–æ¨¡çµ„è¦†å¯«æˆ–å‚³å…¥è‡ªå®šç¾©åˆ—è¡¨)
DEFAULT_EMBEDDING_MODELS: List[str] = [
    EMBED_MODEL_NAME,
    "nomic-ai/nomic-embed-text-v2",
    "jinaai/jina-embeddings-v2-base-zh",
    "Linq-AI-Research/Linq-Embed-Mistral",
]
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# æ¨¡å‹è·¯å¾‘é…ç½®
# æª¢æŸ¥æ˜¯å¦åœ¨ Docker å®¹å™¨å…§ (é€éæ›è¼‰çš„ /models ç›®éŒ„)
if Path("/models").exists() and Path("/models").is_dir():
    # Docker å®¹å™¨å…§ï¼Œä½¿ç”¨æ›è¼‰çš„è·¯å¾‘
    RAG_MODELS_DIR = Path("/models")
    RAG_CACHE_DIR = RAG_MODELS_DIR / "cache"
    RAG_EMBEDDING_DIR = RAG_MODELS_DIR / "embedding"
else:
    # æœ¬åœ°é–‹ç™¼ç’°å¢ƒï¼Œä½¿ç”¨ç›¸å°è·¯å¾‘
    RAG_MODELS_DIR = Path(__file__).parent / "models"
    RAG_CACHE_DIR = RAG_MODELS_DIR / "cache"
    RAG_EMBEDDING_DIR = RAG_MODELS_DIR / "embedding"

# ç¢ºä¿æ¨¡å‹ç›®éŒ„å­˜åœ¨
RAG_MODELS_DIR.mkdir(exist_ok=True)
RAG_CACHE_DIR.mkdir(exist_ok=True)
RAG_EMBEDDING_DIR.mkdir(exist_ok=True)

# å¼·åˆ¶è¨­ç½® HuggingFace ç·©å­˜è·¯å¾‘åˆ°å°ˆæ¡ˆç›®éŒ„
os.environ['HF_HOME'] = str(RAG_CACHE_DIR)
os.environ['TRANSFORMERS_CACHE'] = str(RAG_CACHE_DIR)
os.environ['TORCH_HOME'] = str(RAG_CACHE_DIR)
os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(RAG_EMBEDDING_DIR)

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ----------------- è¼‰å…¥ç´¢å¼• -----------------
index: VectorStoreIndex | None = None

def suggest_similar_models(query: str, limit: int = 5) -> List[str]:
    """Search Hugging Face Hub for models with names similar to the query."""
    try:
        api = HfApi()
        # Search both by repo id and tags in full-text mode
        results = api.search_models(full_text_search=query, limit=limit)
        return [item.modelId for item in results]
    except Exception as e:
        logging.debug(f"ğŸ” Hugging Face search failed: {e}")
        return []


def check_and_download_embedding_model(model_name: str = EMBED_MODEL_NAME, max_retries: int = 3) -> bool:
    """Checks and downloads the embedding model, retrying on failure."""
    os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(RAG_EMBEDDING_DIR)
    
    for attempt in range(max_retries):
        try:
            logging.info(f"Attempt {attempt + 1}/{max_retries}: Loading embedding model '{model_name}'.")
            # LlamaIndex's HuggingFaceEmbedding should use the cache path set by the env var.
            embed_model = HuggingFaceEmbedding(model_name=model_name, device=DEVICE, cache_folder=str(RAG_EMBEDDING_DIR))
            logging.info("âœ… Embedding model loaded successfully.")
            return True
        except Exception as e:
            logging.warning(f"âš ï¸ Failed to load model on attempt {attempt + 1}: {e}")
            if "MetadataIncompleteBuffer" in str(e) or "SafetensorsError" in str(e) or "not found" in str(e).lower():
                 logging.info("Corrupted or incomplete model detected. Attempting to re-download.")
                 try:
                    from sentence_transformers import SentenceTransformer
                    # This will download the model to the cache directory
                    SentenceTransformer(model_name, cache_folder=str(RAG_EMBEDDING_DIR))
                    logging.info("âœ… Model re-downloaded successfully.")
                    # After downloading, try to load it again in the next loop iteration.
                 except Exception as download_error:
                    logging.error(f"âŒ Failed to re-download model: {download_error}")
            
            if attempt < max_retries - 1:
                logging.info("Retrying in 5 seconds...")
                import time
                time.sleep(5)
    
    logging.error(f"âŒ Failed to load embedding model after {max_retries} attempts.")

    # é¡å¤–æ­¥é©Ÿ: å˜—è©¦åœ¨ Hugging Face Hub æœå°‹ç›¸ä¼¼æ¨¡å‹åç¨±ä¸¦æä¾›å»ºè­°
    similar = suggest_similar_models(model_name)
    if similar:
        logging.info("ğŸ” æ‰¾åˆ°å¯èƒ½çš„ç›¸ä¼¼æ¨¡å‹åç¨±ï¼Œè«‹ç¢ºèªæ˜¯å¦æ‹¼å¯«éŒ¯èª¤æˆ–é¸æ“‡ä¸‹åˆ—å…¶ä¸­ä¹‹ä¸€ï¼š")
        for s in similar:
            logging.info(f"  â€¢ {s}")
    else:
        logging.info("ğŸ” æœªåœ¨ Hugging Face Hub æ‰¾åˆ°ç›¸ä¼¼æ¨¡å‹åç¨±ï¼Œè«‹å†æ¬¡ç¢ºèªè¼¸å…¥æ˜¯å¦æ­£ç¢ºã€‚")

    return False


def check_and_download_embedding_models(model_names: List[str] | None = None) -> tuple[list[str], list[str]]:
    """Bulk check/download for a list of embedding models.

    Returns (ok_models, failed_models) lists.
    """
    if model_names is None:
        model_names = DEFAULT_EMBEDDING_MODELS

    ok_models: list[str] = []
    failed_models: list[str] = []
    for name in model_names:
        logging.info(f"ğŸ” æª¢æŸ¥æ¨¡å‹ {name} ...")
        if check_and_download_embedding_model(name):
            ok_models.append(name)
        else:
            failed_models.append(name)

    return ok_models, failed_models

def load_index() -> VectorStoreIndex:
    """å¾å·²å­˜åœ¨çš„ ChromaDB collection è¼‰å…¥ç´¢å¼•"""
    # é¦–å…ˆæª¢æŸ¥ embedding æ¨¡å‹
    if not check_and_download_embedding_model():
        raise RuntimeError(f"ç„¡æ³•è¼‰å…¥æˆ–ä¸‹è¼‰ embedding æ¨¡å‹: {EMBED_MODEL_NAME}")
    
    if not CHROMA_DIR.exists() or not any(CHROMA_DIR.iterdir()):
        raise FileNotFoundError(f"ChromaDB ç›®éŒ„ {CHROMA_DIR} ä¸å­˜åœ¨æˆ–ç‚ºç©ºï¼Œè«‹å…ˆå»ºç½®ç´¢å¼•ï¼")

    try:
        # ä½¿ç”¨ PersistentClient è€Œé HttpClient
        client = chromadb.PersistentClient(path=str(CHROMA_DIR))
        logging.info(f"ChromaDB å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨è·¯å¾‘: {CHROMA_DIR}")
    except Exception as e:
        logging.error(f"ChromaDB å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
        # å˜—è©¦ä½¿ç”¨å‚™ç”¨æ–¹æ³•åˆå§‹åŒ–
        client = chromadb.Client()
        logging.info("ä½¿ç”¨å‚™ç”¨æ–¹æ³•åˆå§‹åŒ– ChromaDB å®¢æˆ¶ç«¯")
        
    # å‰µå»ºæˆ–ç²å–é›†åˆ
    try:
        # å…ˆæª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        try:
            collection = client.get_collection(name=COLLECTION_NAME)
            logging.info(f"ç²å–ç¾æœ‰é›†åˆ: {COLLECTION_NAME}")
        except Exception:
            # å¦‚æœä¸å­˜åœ¨å‰‡å‰µå»º
            collection = client.create_collection(name=COLLECTION_NAME)
            logging.info(f"å‰µå»ºæ–°é›†åˆ: {COLLECTION_NAME}")
            
        # ä½¿ç”¨é›†åˆå‰µå»º vector_store
        vector_store = ChromaVectorStore(
            chroma_collection=collection,
            collection_name=COLLECTION_NAME
        )
    except Exception as e:
        logging.error(f"å‰µå»ºå‘é‡å­˜å„²å¤±æ•—: {e}")
        # å˜—è©¦ç›´æ¥ä½¿ç”¨ client å‰µå»º
        vector_store = ChromaVectorStore(
            chroma_client=client,
            collection_name=COLLECTION_NAME
        )

    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL_NAME, device=DEVICE, cache_folder=str(RAG_EMBEDDING_DIR))

    # å¾ vector_store å»ºç«‹ VectorStoreIndex
    idx = VectorStoreIndex.from_vector_store(vector_store=vector_store, embed_model=embed_model)
    return idx

@app.on_event("startup")
def _startup():
    global index
    try:
        logging.info("æ­£åœ¨å˜—è©¦è¼‰å…¥ç´¢å¼•...")
        index = load_index()
        logging.info("âœ… Index è¼‰å…¥å®Œæˆï¼Œè£ç½®: %s", DEVICE)
    except Exception as e:
        logging.warning("âš ï¸ ç„¡æ³•è¼‰å…¥ç´¢å¼• (é€™æ˜¯æ­£å¸¸çš„ï¼Œå¦‚æœé‚„æ²’å»ºç«‹ç´¢å¼•): %s", e)
        index = None

# ----------------- API ç«¯é» -----------------
@app.post("/query", response_model=QueryResponse)
async def query_endpoint(req: QueryRequest):
    global index
    if index is None:
        # å˜—è©¦å³æ™‚è¼‰å…¥ç´¢å¼•ï¼Œä»¥é¿å…ä½¿ç”¨è€…å·²å»ºå¥½ç´¢å¼•ä½†å°šæœªé‡å•Ÿæœå‹™
        try:
            logging.info("âš ï¸ æŸ¥è©¢æ™‚ index ç‚º Noneï¼Œå˜—è©¦é‡æ–°è¼‰å…¥ç´¢å¼• ...")
            index = load_index()
            logging.info("âœ… é‡æ–°è¼‰å…¥ç´¢å¼•æˆåŠŸï¼")
        except Exception as e:
            logging.warning("âŒ é‡æ–°è¼‰å…¥ç´¢å¼•å¤±æ•—: %s", e)
            raise HTTPException(status_code=503, detail="Index å°šæœªå°±ç·’ï¼Œè«‹ç¨å¾Œæˆ–å…ˆå»ºç½®ç´¢å¼•")

    # æ”¹ç”¨ç´”å‘é‡æª¢ç´¢ï¼Œé¿å…éœ€è¦ LLM/OpenAI API KEY
    # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶åç¨±ï¼Œå‰‡ä½¿ç”¨å…ƒæ•¸æ“šéæ¿¾
    if req.filenames:
        from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator
        
        # å‰µå»ºæ–‡ä»¶åéæ¿¾å™¨
        filters = MetadataFilters(
            filters=[
                MetadataFilter(
                    key="filename",
                    value=req.filenames,
                    operator=FilterOperator.IN
                )
            ]
        )
        retriever = index.as_retriever(similarity_top_k=req.top_k, filters=filters)
    else:
        retriever = index.as_retriever(similarity_top_k=req.top_k)
    
    nodes = retriever.retrieve(req.query)

    if not nodes:
        raise HTTPException(status_code=404, detail="æŸ¥ç„¡ç›¸é—œå…§å®¹")

    # ä»¥æœ€é«˜åˆ†ç¯€é»å…§å®¹åšç‚ºç°¡æ˜“å›ç­”
    answer_text = nodes[0].node.text.strip()

    sources: list[SourceItem] = []
    context_text = ""
    for n in nodes:
        sources.append(
            SourceItem(
                filename=n.metadata.get("filename", ""),
                snippet=n.node.text[:120].strip().replace("\n", " "),
                score=round(float(n.score or 0), 4),
            )
        )
        # æ”¶é›†ä¸Šä¸‹æ–‡ç”¨æ–¼AIå¢å¼·å›ç­”
        context_text += f"\næ–‡æª”ï¼š{n.metadata.get('filename', 'æœªçŸ¥')}\nå…§å®¹ï¼š{n.node.text}\n"

    # å˜—è©¦ä½¿ç”¨AIå¢å¼·å›ç­”
    enhanced_answer = ""
    answer_source = "retrieval"
    
    try:
        from openai_client import openai_client
        ai_result = await openai_client.generate_answer(req.query, context_text)
        
        if ai_result["success"]:
            enhanced_answer = ai_result["answer"]
            answer_source = "ai_enhanced"
            logging.info("âœ… AIå¢å¼·å›ç­”ç”ŸæˆæˆåŠŸ")
        else:
            logging.warning(f"AIå¢å¼·å›ç­”å¤±æ•—: {ai_result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
    except Exception as e:
        logging.warning(f"AIå¢å¼·å›ç­”ç•°å¸¸: {e}")

    return QueryResponse(
        answer=answer_text, 
        sources=sources,
        enhanced_answer=enhanced_answer,
        answer_source=answer_source
    )

@app.get("/indexed_files")
def get_indexed_files():
    """ç²å–å·²ç´¢å¼•çš„æ–‡ä»¶åˆ—è¡¨"""
    global index
    if index is None:
        return {"files": []}
    
    try:
        # å¾å‘é‡å­˜å„²ä¸­ç²å–æ‰€æœ‰æ–‡æª”çš„å…ƒæ•¸æ“š
        vector_store = index.vector_store
        if hasattr(vector_store, '_collection'):
            # å°æ–¼ ChromaVectorStoreï¼Œç›´æ¥æŸ¥è©¢é›†åˆ
            collection = vector_store._collection
            result = collection.get(include=['metadatas'])
            
            # æå–å”¯ä¸€çš„æ–‡ä»¶å
            filenames = set()
            if result and 'metadatas' in result:
                for metadata in result['metadatas']:
                    if metadata and 'filename' in metadata:
                        filenames.add(metadata['filename'])
            
            return {"files": sorted(list(filenames))}
        else:
            # å‚™ç”¨æ–¹æ³•ï¼šé€šéæŸ¥è©¢ç²å–
            retriever = index.as_retriever(similarity_top_k=100)  # ç²å–è¼ƒå¤šçµæœ
            nodes = retriever.retrieve("*")  # ä½¿ç”¨é€šé…ç¬¦æŸ¥è©¢
            
            filenames = set()
            for node in nodes:
                if hasattr(node, 'metadata') and 'filename' in node.metadata:
                    filenames.add(node.metadata['filename'])
            
            return {"files": sorted(list(filenames))}
    except Exception as e:
        logging.error(f"ç²å–å·²ç´¢å¼•æ–‡ä»¶åˆ—è¡¨å¤±æ•—: {e}")
        return {"files": []}

@app.get("/health")
def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»ï¼Œè¿”å›APIç‹€æ…‹å’Œé…ç½®ä¿¡æ¯"""
    try:
        from config_manager import ConfigManager
        config_manager = ConfigManager()
        api_status = config_manager.get_api_status()
    except Exception:
        api_status = {"configured": False}
        
    return {
        "status": "healthy",
        "index_loaded": index is not None,
        "device": DEVICE,
        "embed_model": EMBED_MODEL_NAME,
        "chroma_dir": str(CHROMA_DIR),
        "collection_name": COLLECTION_NAME,
        "default_top_k": 5,
        "max_top_k": 20,
        "api_configured": api_status.get("configured", False)
    }
