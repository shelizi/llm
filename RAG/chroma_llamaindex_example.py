"""
ä½œè€…: Cascade
ç¯„ä¾‹: å°‡æ–‡ä»¶ä½¿ç”¨ LlamaIndex æ­é…æŒ‡å®š Embedding Model
      `intfloat/multilingual-e5-large-instruct` å¯«å…¥ ChromaDBï¼Œä¸¦ç¤ºç¯„æŸ¥è©¢ã€‚

ä½¿ç”¨èªªæ˜ï¼š
1. å®‰è£å¥—ä»¶ (å»ºè­° python >=3.9)
   pip install llama-index chromadb sentence-transformers accelerate
2. åŸ·è¡Œç¯„ä¾‹
   python chroma_llamaindex_example.py
3. åŸ·è¡Œå¾Œï¼Œç¨‹å¼æœƒå°‡ä¸‰æ®µç¤ºç¯„æ–‡å­—å¯«å…¥ ChromaDBï¼Œä¹‹å¾Œå¯åœ¨çµ‚ç«¯æ©Ÿè¼¸å…¥è‡ªç„¶èªè¨€æŸ¥è©¢å…§å®¹
   (ç›´æ¥æŒ‰ Enter çµæŸç¨‹å¼)ã€‚

ç¨‹å¼åŸå‰‡ï¼š
- æ‰€æœ‰è¨»è§£çš†ä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡
- åš´è¬¹ä½†ç°¡æ½”çš„ logging æ–¹ä¾¿è¿½è¹¤æµç¨‹
"""

import logging
from pathlib import Path
import os

import chromadb
from llama_index.core import Document, StorageContext, VectorStoreIndex
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
import torch

# è¨­å®š logging æ ¼å¼
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


def check_and_download_embedding_model(model_name: str, embedding_dir: Path) -> bool:
    """æª¢æŸ¥ä¸¦ä¸‹è¼‰ embedding æ¨¡å‹åˆ°å°ˆæ¡ˆç›®éŒ„ï¼Œå¦‚æœéœ€è¦çš„è©±"""
    try:
        logging.info(f"ğŸ” æª¢æŸ¥ embedding æ¨¡å‹: {model_name}")
        logging.info(f"ğŸ“ æ¨¡å‹å°‡ä¸‹è¼‰åˆ°: {embedding_dir}")
        
        # é¦–å…ˆå˜—è©¦ç›´æ¥è¼‰å…¥æ¨¡å‹ä¾†æª¢æŸ¥æ˜¯å¦å¯ç”¨
        try:
            device = "cuda" if torch.cuda.is_available() else "cpu"
            # å˜—è©¦å‰µå»º embedding æ¨¡å‹å¯¦ä¾‹
            embed_model = HuggingFaceEmbedding(model_name=model_name, device=device)
            logging.info(f"âœ… Embedding æ¨¡å‹ {model_name} å·²å¯ç”¨")
            return True
            
        except Exception as e:
            logging.warning(f"âš ï¸ Embedding æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            logging.info(f"ğŸ”„ æ­£åœ¨ä¸‹è¼‰ embedding æ¨¡å‹åˆ°å°ˆæ¡ˆç›®éŒ„: {model_name}")
            
            # ä½¿ç”¨ sentence-transformers æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹åˆ°æŒ‡å®šç›®éŒ„
            try:
                from sentence_transformers import SentenceTransformer
                
                # ç¢ºä¿ç’°å¢ƒè®Šæ•¸å·²è¨­ç½®
                os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(embedding_dir)
                
                # ä¸‹è¼‰æ¨¡å‹åˆ°å°ˆæ¡ˆç›®éŒ„
                logging.info(f"ğŸ“¥ é–‹å§‹ä¸‹è¼‰æ¨¡å‹åˆ°: {embedding_dir}")
                model = SentenceTransformer(model_name, cache_folder=str(embedding_dir))
                logging.info(f"âœ… æˆåŠŸä¸‹è¼‰ embedding æ¨¡å‹åˆ°å°ˆæ¡ˆç›®éŒ„: {model_name}")
                
                # å†æ¬¡æ¸¬è©¦ LlamaIndex çš„ HuggingFaceEmbedding
                embed_model = HuggingFaceEmbedding(model_name=model_name, device=device)
                logging.info(f"âœ… Embedding æ¨¡å‹ {model_name} ç¾åœ¨å¯ä»¥æ­£å¸¸ä½¿ç”¨")
                return True
                
            except Exception as download_error:
                logging.error(f"âŒ ä¸‹è¼‰ embedding æ¨¡å‹å¤±æ•—: {download_error}")
                return False
                
    except ImportError as e:
        logging.error(f"âŒ ç¼ºå°‘å¿…è¦çš„å¥—ä»¶: {e}")
        logging.info("è«‹é‹è¡Œ: pip install sentence-transformers llama-index-embeddings-huggingface")
        return False

def build_index() -> VectorStoreIndex:
    """å»ºç«‹å‘é‡ç´¢å¼•ä¸¦å°‡æ–‡ä»¶å¯«å…¥ ChromaDB"""

    # 0. è¨­ç½®æ¨¡å‹è·¯å¾‘é…ç½®
    base_dir = Path(__file__).parent
    models_dir = base_dir / "models"
    cache_dir = models_dir / "cache"
    embedding_dir = models_dir / "embedding"
    
    # å‰µå»ºå¿…è¦ç›®éŒ„
    models_dir.mkdir(exist_ok=True)
    cache_dir.mkdir(exist_ok=True)
    embedding_dir.mkdir(exist_ok=True)
    
    # å¼·åˆ¶è¨­ç½® HuggingFace ç·©å­˜è·¯å¾‘åˆ°å°ˆæ¡ˆç›®éŒ„
    os.environ['HF_HOME'] = str(cache_dir)
    os.environ['TRANSFORMERS_CACHE'] = str(cache_dir)
    os.environ['TORCH_HOME'] = str(cache_dir)
    os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(embedding_dir)
    logging.info("è¨­ç½®æ¨¡å‹ç·©å­˜è·¯å¾‘: %s", cache_dir)
    logging.info("è¨­ç½® embedding æ¨¡å‹è·¯å¾‘: %s", embedding_dir)

    # 1. å»ºç«‹ HuggingFace Embedding æ¨¡å‹
    embed_model_name = "intfloat/multilingual-e5-large-instruct"
    logging.info("è¼‰å…¥ Embedding æ¨¡å‹: %s", embed_model_name)
    
    # æª¢æŸ¥ä¸¦ä¸‹è¼‰ embedding æ¨¡å‹
    if not check_and_download_embedding_model(embed_model_name, embedding_dir):
        raise RuntimeError(f"ç„¡æ³•è¼‰å…¥æˆ–ä¸‹è¼‰ embedding æ¨¡å‹: {embed_model_name}")
    
    # è‡ªå‹•åµæ¸¬ GPUï¼Œè‹¥ç„¡å‰‡å›é€€è‡³ CPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logging.info("ä½¿ç”¨è£ç½®: %s", device)
    embed_model = HuggingFaceEmbedding(model_name=embed_model_name, device=device)

    # 2. æº–å‚™æ–‡ä»¶è³‡æ–™ (å¯è‡ªè¡Œæ›¿æ›ç‚ºä»»æ„æ–‡æœ¬)
    texts = [
        "å°ç£æ˜¯ä½æ–¼æ±äºçš„å³¶å¶¼ï¼Œé¦–éƒ½ç‚ºè‡ºåŒ—å¸‚ã€‚",
        "é«˜é›„å¸‚æ˜¯å°ç£çš„æµ·æ¸¯åŸå¸‚ï¼Œä»¥ç†±æƒ…çš„æ¸¯éƒ½èåã€‚",
        "é˜¿é‡Œå±±çš„æ—¥å‡ºèˆ‡é›²æµ·æ˜¯ä¾†å°æ—…éŠä¸å¯éŒ¯éçš„æ™¯é»ã€‚",
    ]
    documents = [Document(text=t) for t in texts]

    # 3. å»ºç«‹ ChromaDB Client èˆ‡ Vector Store (è³‡æ–™æŒä¹…åŒ–è‡³ ./RAG/chroma_db)
    persist_dir = Path(__file__).parent / "chroma_db"
    persist_dir.mkdir(exist_ok=True)
    
    try:
        # ä½¿ç”¨ PersistentClient è€Œé HttpClient
        client = chromadb.PersistentClient(path=str(persist_dir))
        logging.info(f"ChromaDB å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨è·¯å¾‘: {persist_dir}")
    except Exception as e:
        logging.error(f"ChromaDB å®¢æˆ¶ç«¯åˆå§‹åŒ–å¤±æ•—: {e}")
        # å˜—è©¦ä½¿ç”¨å‚™ç”¨æ–¹æ³•åˆå§‹åŒ–
        client = chromadb.Client()
        logging.info("ä½¿ç”¨å‚™ç”¨æ–¹æ³•åˆå§‹åŒ– ChromaDB å®¢æˆ¶ç«¯")
        
    # å‰µå»ºæˆ–ç²å–é›†åˆ
    collection_name = "taiwan_demo"
    try:
        # å…ˆæª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        try:
            collection = client.get_collection(name=collection_name)
            logging.info(f"ç²å–ç¾æœ‰é›†åˆ: {collection_name}")
        except Exception:
            # å¦‚æœä¸å­˜åœ¨å‰‡å‰µå»º
            collection = client.create_collection(name=collection_name)
            logging.info(f"å‰µå»ºæ–°é›†åˆ: {collection_name}")
            
        # ä½¿ç”¨é›†åˆå‰µå»º vector_store
        vector_store = ChromaVectorStore(
            chroma_collection=collection,
            collection_name=collection_name
        )
    except Exception as e:
        logging.error(f"å‰µå»ºå‘é‡å­˜å„²å¤±æ•—: {e}")
        # å˜—è©¦ç›´æ¥ä½¿ç”¨ client å‰µå»º
        vector_store = ChromaVectorStore(
            chroma_client=client,
            collection_name=collection_name
        )

    # 4. å»ºç«‹ StorageContext ä¸¦å°‡æ–‡ä»¶å¯«å…¥
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    index = VectorStoreIndex.from_documents(
        documents,
        storage_context=storage_context,
        embed_model=embed_model,
    )
    logging.info("âœ… æ–‡ä»¶å·²æˆåŠŸå¯«å…¥ ChromaDB (è·¯å¾‘: %s)", persist_dir)
    return index


def query_loop(index: VectorStoreIndex) -> None:
    """äº’å‹•å¼æŸ¥è©¢è¿´åœˆï¼Œå¯å¤šæ¬¡è¼¸å…¥è‡ªç„¶èªè¨€æŸ¥è©¢"""

    query_engine = index.as_query_engine(similarity_top_k=3)
    print("\n===== äº’å‹•å¼æŸ¥è©¢é–‹å§‹ï¼ŒæŒ‰ Enter ç›´æ¥çµæŸ =====")
    while True:
        query = input("è«‹è¼¸å…¥æŸ¥è©¢å…§å®¹: ").strip()
        if not query:
            print("æŸ¥è©¢çµæŸï¼Œå†è¦‹ï¼")
            break

        logging.info("é–‹å§‹æŸ¥è©¢: %s", query)
        response = query_engine.query(query)
        print("\n----- æŸ¥è©¢çµæœ -----")
        print(response.response)  # LlamaIndex Response ç‰©ä»¶
        print("--------------------\n")


if __name__ == "__main__":
    # è‹¥å·²æœ‰æŒä¹…åŒ–è³‡æ–™ï¼Œå¯é¸æ“‡ç•¥éé‡å»º (æ­¤è™•ç‚ºç¤ºç¯„ç›´æ¥é‡å»º)
    idx = build_index()
    query_loop(idx)
